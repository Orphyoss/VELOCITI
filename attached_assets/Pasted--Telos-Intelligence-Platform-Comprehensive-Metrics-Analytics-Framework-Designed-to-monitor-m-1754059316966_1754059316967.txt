"""
Telos Intelligence Platform - Comprehensive Metrics & Analytics Framework
Designed to monitor, measure, and optimize AI-driven airline intelligence

Think of this as the "mission control dashboard" for your AI agents - providing
real-time visibility into system performance, business impact, and ROI validation.
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta, date
from typing import Dict, List, Optional, Union, Any, Tuple
import json
import logging
from dataclasses import dataclass
from enum import Enum
import asyncio
from scipy import stats
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# METRICS TAXONOMY AND DEFINITIONS
# ============================================================================

class MetricCategory(str, Enum):
    SYSTEM_PERFORMANCE = "System Performance"
    AI_ACCURACY = "AI Accuracy & Quality"
    BUSINESS_IMPACT = "Business Impact & ROI"
    USER_ADOPTION = "User Adoption & Satisfaction"
    DATA_QUALITY = "Data Quality & Reliability"
    OPERATIONAL_EFFICIENCY = "Operational Efficiency"

class MetricFrequency(str, Enum):
    REAL_TIME = "Real-time"
    HOURLY = "Hourly"
    DAILY = "Daily"
    WEEKLY = "Weekly"
    MONTHLY = "Monthly"
    QUARTERLY = "Quarterly"

@dataclass
class MetricDefinition:
    """Standard metric definition with business context and measurement methodology"""
    metric_name: str
    category: MetricCategory
    frequency: MetricFrequency
    description: str
    calculation_method: str
    target_value: Union[float, str]
    threshold_warning: Union[float, str]
    threshold_critical: Union[float, str]
    business_impact: str
    data_sources: List[str]

# ============================================================================
# CORE METRICS REGISTRY
# ============================================================================

class TelosMetricsRegistry:
    """Central registry of all Telos platform metrics with business context"""
    
    def __init__(self):
        self.metrics = self._initialize_metrics_catalog()
        
    def _initialize_metrics_catalog(self) -> Dict[str, MetricDefinition]:
        """Initialize comprehensive metrics catalog"""
        
        return {
            # ================================================================
            # SYSTEM PERFORMANCE METRICS
            # ================================================================
            "nightshift_processing_time": MetricDefinition(
                metric_name="NightShift Processing Time",
                category=MetricCategory.SYSTEM_PERFORMANCE,
                frequency=MetricFrequency.DAILY,
                description="Total time for overnight intelligence processing to complete",
                calculation_method="processing_end_time - processing_start_time",
                target_value="< 45 minutes",
                threshold_warning="60 minutes",
                threshold_critical="90 minutes",
                business_impact="Longer processing delays morning briefings and reduces analyst productivity",
                data_sources=["nightshift_processing", "system_logs"]
            ),
            
            "system_availability": MetricDefinition(
                metric_name="System Availability",
                category=MetricCategory.SYSTEM_PERFORMANCE,
                frequency=MetricFrequency.REAL_TIME,
                description="Percentage of time system is operational and accessible",
                calculation_method="(uptime_seconds / total_seconds) * 100",
                target_value="> 99.9%",
                threshold_warning="99.5%",
                threshold_critical="99.0%",
                business_impact="System downtime directly impacts analyst productivity and decision-making capability",
                data_sources=["system_health_checks", "error_logs", "databricks_metrics"]
            ),
            
            "data_freshness": MetricDefinition(
                metric_name="Data Freshness",
                category=MetricCategory.DATA_QUALITY,
                frequency=MetricFrequency.HOURLY,
                description="Time elapsed since last successful data update from external sources",
                calculation_method="current_time - last_successful_update",
                target_value="< 2 hours",
                threshold_warning="4 hours",
                threshold_critical="8 hours",
                business_impact="Stale data leads to outdated insights and poor decision-making",
                data_sources=["data_ingestion_logs", "infare_api", "oag_feeds"]
            ),
            
            # ================================================================
            # AI ACCURACY & QUALITY METRICS
            # ================================================================
            "insight_accuracy_rate": MetricDefinition(
                metric_name="AI Insight Accuracy Rate",
                category=MetricCategory.AI_ACCURACY,
                frequency=MetricFrequency.WEEKLY,
                description="Percentage of AI-generated insights validated as accurate by analysts",
                calculation_method="(accurate_insights / total_insights_validated) * 100",
                target_value="> 85%",
                threshold_warning="80%",
                threshold_critical="75%",
                business_impact="Low accuracy erodes analyst trust and reduces system adoption",
                data_sources=["analyst_interactions", "insight_feedback", "validation_tracking"]
            ),
            
            "competitive_alert_precision": MetricDefinition(
                metric_name="Competitive Alert Precision",
                category=MetricCategory.AI_ACCURACY,
                frequency=MetricFrequency.DAILY,
                description="Percentage of competitive alerts that are actionable and accurate",
                calculation_method="(actionable_competitive_alerts / total_competitive_alerts) * 100",
                target_value="> 80%",
                threshold_warning="70%",
                threshold_critical="60%",
                business_impact="False competitive alerts waste analyst time and create alert fatigue",
                data_sources=["intelligence_insights", "competitive_pricing", "analyst_feedback"]
            ),
            
            "prediction_confidence_distribution": MetricDefinition(
                metric_name="AI Prediction Confidence Distribution",
                category=MetricCategory.AI_ACCURACY,
                frequency=MetricFrequency.WEEKLY,
                description="Distribution of confidence scores across all AI predictions",
                calculation_method="histogram(confidence_scores, bins=[0.7, 0.8, 0.85, 0.9, 0.95, 1.0])",
                target_value="> 70% of predictions with confidence > 0.8",
                threshold_warning="< 60% high confidence",
                threshold_critical="< 50% high confidence",
                business_impact="Low confidence predictions indicate model uncertainty and require human validation",
                data_sources=["intelligence_insights", "ml_model_outputs"]
            ),
            
            # ================================================================
            # BUSINESS IMPACT & ROI METRICS
            # ================================================================
            "analyst_time_savings": MetricDefinition(
                metric_name="Analyst Time Savings per Day",
                category=MetricCategory.BUSINESS_IMPACT,
                frequency=MetricFrequency.DAILY,
                description="Minutes saved per analyst through automated intelligence vs manual analysis",
                calculation_method="baseline_analysis_time - current_analysis_time",
                target_value="> 60 minutes/day",
                threshold_warning="< 45 minutes/day",
                threshold_critical="< 30 minutes/day",
                business_impact="Time savings directly correlate to productivity gains and ROI",
                data_sources=["analyst_interactions", "time_tracking", "productivity_surveys"]
            ),
            
            "revenue_impact_tracking": MetricDefinition(
                metric_name="Revenue Impact from AI Decisions",
                category=MetricCategory.BUSINESS_IMPACT,
                frequency=MetricFrequency.MONTHLY,
                description="Estimated revenue impact from decisions driven by AI insights",
                calculation_method="sum(revenue_uplift_from_ai_driven_decisions)",
                target_value="> €500K/month",
                threshold_warning="< €300K/month",
                threshold_critical="< €200K/month",
                business_impact="Revenue impact validates ROI and justifies platform investment",
                data_sources=["flight_performance", "pricing_actions", "load_factor_improvements"]
            ),
            
            "competitive_response_speed": MetricDefinition(
                metric_name="Competitive Response Speed",
                category=MetricCategory.BUSINESS_IMPACT,
                frequency=MetricFrequency.WEEKLY,
                description="Average time from competitor price change to EasyJet response action",
                calculation_method="avg(easyjet_action_time - competitor_price_change_time)",
                target_value="< 4 hours",
                threshold_warning="> 8 hours",
                threshold_critical="> 24 hours",
                business_impact="Faster competitive response protects market share and revenue",
                data_sources=["competitive_pricing", "rm_pricing_actions", "intelligence_insights"]
            ),
            
            # ================================================================
            # USER ADOPTION & SATISFACTION METRICS
            # ================================================================
            "daily_active_users": MetricDefinition(
                metric_name="Daily Active Users",
                category=MetricCategory.USER_ADOPTION,
                frequency=MetricFrequency.DAILY,
                description="Number of unique analysts using the system daily",
                calculation_method="count(distinct(analyst_id)) where login_date = current_date",
                target_value="> 90% of target analysts",
                threshold_warning="< 80%",
                threshold_critical="< 70%",
                business_impact="Low adoption reduces ROI and indicates user experience issues",
                data_sources=["analyst_interactions", "system_access_logs"]
            ),
            
            "user_satisfaction_score": MetricDefinition(
                metric_name="User Satisfaction Score (NPS)",
                category=MetricCategory.USER_ADOPTION,
                frequency=MetricFrequency.MONTHLY,
                description="Net Promoter Score based on analyst feedback surveys",
                calculation_method="(promoters - detractors) / total_responses * 100",
                target_value="> 50 NPS",
                threshold_warning="< 30 NPS",
                threshold_critical="< 10 NPS",
                business_impact="User satisfaction drives adoption and long-term success",
                data_sources=["user_surveys", "feedback_forms", "satisfaction_ratings"]
            ),
            
            "insight_action_rate": MetricDefinition(
                metric_name="Insight Action Rate",
                category=MetricCategory.USER_ADOPTION,
                frequency=MetricFrequency.WEEKLY,
                description="Percentage of AI insights that result in analyst action",
                calculation_method="(insights_acted_upon / total_insights_presented) * 100",
                target_value="> 60%",
                threshold_warning="< 50%",
                threshold_critical="< 40%",
                business_impact="Low action rate indicates insights are not valuable or actionable",
                data_sources=["intelligence_insights", "analyst_interactions", "action_tracking"]
            ),
            
            # ================================================================
            # DATA QUALITY & RELIABILITY METRICS
            # ================================================================
            "data_completeness_rate": MetricDefinition(
                metric_name="Data Completeness Rate",
                category=MetricCategory.DATA_QUALITY,
                frequency=MetricFrequency.DAILY,
                description="Percentage of expected data records received from external sources",
                calculation_method="(records_received / records_expected) * 100",
                target_value="> 95%",
                threshold_warning="< 90%",
                threshold_critical="< 85%",
                business_impact="Incomplete data leads to biased insights and missed opportunities",
                data_sources=["data_ingestion_logs", "infare_feeds", "oag_data", "search_data"]
            ),
            
            "data_accuracy_score": MetricDefinition(
                metric_name="External Data Accuracy Score",
                category=MetricCategory.DATA_QUALITY,
                frequency=MetricFrequency.WEEKLY,
                description="Accuracy of external data validated against known benchmarks",
                calculation_method="(validated_accurate_records / total_validated_records) * 100",
                target_value="> 98%",
                threshold_warning="< 95%",
                threshold_critical="< 92%",
                business_impact="Inaccurate source data propagates errors through all AI insights",
                data_sources=["data_validation_checks", "competitive_pricing", "benchmark_comparisons"]
            ),
            
            # ================================================================
            # OPERATIONAL EFFICIENCY METRICS
            # ================================================================
            "alert_fatigue_index": MetricDefinition(
                metric_name="Alert Fatigue Index",
                category=MetricCategory.OPERATIONAL_EFFICIENCY,
                frequency=MetricFrequency.WEEKLY,
                description="Ratio of actionable alerts to total alerts generated",
                calculation_method="total_alerts / actionable_alerts",
                target_value="< 1.5",
                threshold_warning="> 2.0",
                threshold_critical="> 3.0",
                business_impact="Too many alerts create fatigue and reduce analyst responsiveness",
                data_sources=["intelligence_insights", "alert_management", "analyst_feedback"]
            ),
            
            "insight_generation_cost": MetricDefinition(
                metric_name="Cost per Insight Generated",
                category=MetricCategory.OPERATIONAL_EFFICIENCY,
                frequency=MetricFrequency.MONTHLY,
                description="Total system cost divided by number of insights generated",
                calculation_method="(compute_costs + api_costs + storage_costs) / total_insights",
                target_value="< €5 per insight",
                threshold_warning="> €8 per insight",
                threshold_critical="> €12 per insight",
                business_impact="High cost per insight reduces platform profitability and scalability",
                data_sources=["billing_data", "usage_metrics", "intelligence_insights"]
            )
        }
    
    def get_metric(self, metric_name: str) -> Optional[MetricDefinition]:
        """Retrieve metric definition by name"""
        return self.metrics.get(metric_name)
    
    def get_metrics_by_category(self, category: MetricCategory) -> List[MetricDefinition]:
        """Get all metrics in a specific category"""
        return [metric for metric in self.metrics.values() if metric.category == category]

# ============================================================================
# METRICS CALCULATION ENGINE
# ============================================================================

class TelosMetricsCalculator:
    """Core engine for calculating and tracking Telos platform metrics"""
    
    def __init__(self, databricks_connection, logger=None):
        self.db_connection = databricks_connection
        self.logger = logger or logging.getLogger(__name__)
        self.registry = TelosMetricsRegistry()
        
    async def calculate_system_performance_metrics(self, date_range: Tuple[date, date]) -> Dict[str, Any]:
        """Calculate all system performance metrics for given date range"""
        start_date, end_date = date_range
        
        # NightShift Processing Time
        processing_times = await self._query_database(f"""
            SELECT 
                processing_date,
                TIMESTAMPDIFF(MINUTE, processing_start, processing_end) as processing_minutes,
                processing_status
            FROM nightshift_processing 
            WHERE processing_date BETWEEN '{start_date}' AND '{end_date}'
            ORDER BY processing_date DESC
        """)
        
        # System Availability  
        availability_data = await self._query_database(f"""
            SELECT 
                check_date,
                check_hour,
                SUM(CASE WHEN status = 'UP' THEN 1 ELSE 0 END) as uptime_checks,
                COUNT(*) as total_checks
            FROM system_health_checks 
            WHERE check_date BETWEEN '{start_date}' AND '{end_date}'
            GROUP BY check_date, check_hour
        """)
        
        # Data Freshness
        freshness_data = await self._query_database(f"""
            SELECT 
                data_source,
                MAX(last_update_time) as latest_update,
                TIMESTAMPDIFF(HOUR, MAX(last_update_time), NOW()) as hours_since_update
            FROM data_ingestion_status
            WHERE DATE(last_update_time) BETWEEN '{start_date}' AND '{end_date}'
            GROUP BY data_source
        """)
        
        return {
            "nightshift_processing_time": {
                "avg_minutes": processing_times['processing_minutes'].mean(),
                "max_minutes": processing_times['processing_minutes'].max(),
                "success_rate": (processing_times['processing_status'] == 'Completed').mean() * 100,
                "trend": self._calculate_trend(processing_times, 'processing_minutes', 'processing_date')
            },
            "system_availability": {
                "availability_percent": (availability_data['uptime_checks'].sum() / availability_data['total_checks'].sum()) * 100,
                "daily_availability": availability_data.groupby('check_date').apply(
                    lambda x: (x['uptime_checks'].sum() / x['total_checks'].sum()) * 100
                ).to_dict()
            },
            "data_freshness": {
                "avg_hours_delay": freshness_data['hours_since_update'].mean(),
                "max_hours_delay": freshness_data['hours_since_update'].max(),
                "by_source": freshness_data.set_index('data_source')['hours_since_update'].to_dict()
            }
        }
    
    async def calculate_ai_accuracy_metrics(self, date_range: Tuple[date, date]) -> Dict[str, Any]:
        """Calculate AI accuracy and quality metrics"""
        start_date, end_date = date_range
        
        # Insight Accuracy Rate
        accuracy_data = await self._query_database(f"""
            SELECT 
                ii.insight_date,
                ii.insight_type,
                ii.confidence_score,
                ai.satisfaction_rating,
                CASE WHEN ai.satisfaction_rating >= 4 THEN 1 ELSE 0 END as accurate_insight
            FROM intelligence_insights ii
            JOIN analyst_interactions ai ON ii.id = ai.insight_id
            WHERE ii.insight_date BETWEEN '{start_date}' AND '{end_date}'
            AND ai.interaction_type = 'Feedback'
        """)
        
        # Competitive Alert Precision
        competitive_alerts = await self._query_database(f"""
            SELECT 
                ii.insight_date,
                ii.priority_level,
                ai.action_taken,
                CASE WHEN ai.action_taken = TRUE THEN 1 ELSE 0 END as actionable_alert
            FROM intelligence_insights ii
            LEFT JOIN analyst_interactions ai ON ii.id = ai.insight_id
            WHERE ii.insight_date BETWEEN '{start_date}' AND '{end_date}'
            AND ii.agent_source = 'Competitive_Intelligence_Agent'
        """)
        
        # Confidence Score Distribution
        confidence_distribution = await self._query_database(f"""
            SELECT 
                confidence_score,
                COUNT(*) as insight_count,
                CASE 
                    WHEN confidence_score >= 0.9 THEN 'Very High'
                    WHEN confidence_score >= 0.85 THEN 'High'
                    WHEN confidence_score >= 0.8 THEN 'Medium'
                    ELSE 'Low'
                END as confidence_bucket
            FROM intelligence_insights
            WHERE insight_date BETWEEN '{start_date}' AND '{end_date}'
            GROUP BY confidence_score, confidence_bucket
        """)
        
        return {
            "insight_accuracy_rate": {
                "overall_accuracy": accuracy_data['accurate_insight'].mean() * 100,
                "by_insight_type": accuracy_data.groupby('insight_type')['accurate_insight'].mean() * 100,
                "avg_satisfaction": accuracy_data['satisfaction_rating'].mean(),
                "trend": self._calculate_trend(accuracy_data, 'accurate_insight', 'insight_date')
            },
            "competitive_alert_precision": {
                "precision_rate": competitive_alerts['actionable_alert'].mean() * 100,
                "by_priority": competitive_alerts.groupby('priority_level')['actionable_alert'].mean() * 100,
                "total_alerts": len(competitive_alerts)
            },
            "confidence_distribution": {
                "distribution": confidence_distribution.groupby('confidence_bucket')['insight_count'].sum().to_dict(),
                "avg_confidence": confidence_distribution['confidence_score'].mean(),
                "high_confidence_rate": (confidence_distribution['confidence_score'] >= 0.8).mean() * 100
            }
        }
    
    async def calculate_business_impact_metrics(self, date_range: Tuple[date, date]) -> Dict[str, Any]:
        """Calculate business impact and ROI metrics"""
        start_date, end_date = date_range
        
        # Analyst Time Savings
        time_savings = await self._query_database(f"""
            SELECT 
                interaction_date,
                analyst_id,
                SUM(time_saved_minutes) as daily_time_saved
            FROM analyst_interactions
            WHERE DATE(interaction_date) BETWEEN '{start_date}' AND '{end_date}'
            AND time_saved_minutes IS NOT NULL
            GROUP BY interaction_date, analyst_id
        """)
        
        # Revenue Impact Tracking
        revenue_impact = await self._query_database(f"""
            SELECT 
                rpa.action_date,
                rpa.route_id,
                rpa.new_price - rpa.old_price as price_change,
                fp.revenue_total,
                fp.load_factor,
                CASE WHEN rpa.change_reason = 'ai_recommendation' THEN fp.revenue_total ELSE 0 END as ai_driven_revenue
            FROM rm_pricing_actions rpa
            JOIN flight_performance fp ON rpa.route_id = fp.route_id 
                AND rpa.flight_date = fp.flight_date
            WHERE rpa.action_date BETWEEN '{start_date}' AND '{end_date}'
        """)
        
        # Competitive Response Speed
        response_speed = await self._query_database(f"""
            SELECT 
                cp.observation_date,
                cp.route_id,
                cp.airline_code,
                cp.price_amount,
                rpa.action_date,
                TIMESTAMPDIFF(HOUR, cp.observation_date, rpa.action_date) as response_hours
            FROM competitive_pricing cp
            JOIN rm_pricing_actions rpa ON cp.route_id = rpa.route_id
                AND rpa.change_reason = 'competitor_response'
            WHERE cp.observation_date BETWEEN '{start_date}' AND '{end_date}'
            AND cp.airline_code = 'RYR'  -- Focus on Ryanair responses
        """)
        
        return {
            "analyst_time_savings": {
                "avg_daily_savings_minutes": time_savings['daily_time_saved'].mean(),
                "total_hours_saved": time_savings['daily_time_saved'].sum() / 60,
                "by_analyst": time_savings.groupby('analyst_id')['daily_time_saved'].mean().to_dict(),
                "trend": self._calculate_trend(time_savings, 'daily_time_saved', 'interaction_date')
            },
            "revenue_impact": {
                "total_ai_driven_revenue": revenue_impact['ai_driven_revenue'].sum(),
                "avg_price_change": revenue_impact['price_change'].mean(),
                "routes_impacted": revenue_impact['route_id'].nunique(),
                "load_factor_correlation": revenue_impact[['price_change', 'load_factor']].corr().iloc[0,1]
            },
            "competitive_response_speed": {
                "avg_response_hours": response_speed['response_hours'].mean(),
                "median_response_hours": response_speed['response_hours'].median(),
                "responses_under_4h": (response_speed['response_hours'] <= 4).mean() * 100,
                "by_route": response_speed.groupby('route_id')['response_hours'].mean().to_dict()
            }
        }
    
    async def calculate_user_adoption_metrics(self, date_range: Tuple[date, date]) -> Dict[str, Any]:
        """Calculate user adoption and satisfaction metrics"""
        start_date, end_date = date_range
        
        # Daily Active Users
        active_users = await self._query_database(f"""
            SELECT 
                DATE(interaction_date) as usage_date,
                COUNT(DISTINCT analyst_id) as daily_active_users,
                COUNT(*) as total_interactions
            FROM analyst_interactions
            WHERE DATE(interaction_date) BETWEEN '{start_date}' AND '{end_date}'
            GROUP BY DATE(interaction_date)
            ORDER BY usage_date
        """)
        
        # User Satisfaction (from feedback)
        satisfaction_data = await self._query_database(f"""
            SELECT 
                DATE(interaction_date) as feedback_date,
                satisfaction_rating,
                analyst_id,
                interaction_type
            FROM analyst_interactions
            WHERE DATE(interaction_date) BETWEEN '{start_date}' AND '{end_date}'
            AND satisfaction_rating IS NOT NULL
        """)
        
        # Insight Action Rate
        action_rates = await self._query_database(f"""
            SELECT 
                ii.insight_date,
                ii.insight_type,
                ii.priority_level,
                COUNT(*) as total_insights,
                SUM(CASE WHEN ii.action_taken = TRUE THEN 1 ELSE 0 END) as insights_acted_upon
            FROM intelligence_insights ii
            WHERE ii.insight_date BETWEEN '{start_date}' AND '{end_date}'
            GROUP BY ii.insight_date, ii.insight_type, ii.priority_level
        """)
        
        # Calculate NPS from satisfaction ratings
        nps_score = self._calculate_nps(satisfaction_data['satisfaction_rating'])
        
        return {
            "daily_active_users": {
                "avg_daily_users": active_users['daily_active_users'].mean(),
                "peak_daily_users": active_users['daily_active_users'].max(),
                "user_growth_trend": self._calculate_trend(active_users, 'daily_active_users', 'usage_date'),
                "engagement_trend": self._calculate_trend(active_users, 'total_interactions', 'usage_date')
            },
            "user_satisfaction": {
                "nps_score": nps_score,
                "avg_satisfaction": satisfaction_data['satisfaction_rating'].mean(),
                "satisfaction_distribution": satisfaction_data['satisfaction_rating'].value_counts().to_dict(),
                "by_analyst": satisfaction_data.groupby('analyst_id')['satisfaction_rating'].mean().to_dict()
            },
            "insight_action_rate": {
                "overall_action_rate": (action_rates['insights_acted_upon'].sum() / action_rates['total_insights'].sum()) * 100,
                "by_insight_type": (action_rates.groupby('insight_type')['insights_acted_upon'].sum() / 
                                  action_rates.groupby('insight_type')['total_insights'].sum() * 100).to_dict(),
                "by_priority": (action_rates.groupby('priority_level')['insights_acted_upon'].sum() / 
                              action_rates.groupby('priority_level')['total_insights'].sum() * 100).to_dict()
            }
        }
    
    def _calculate_trend(self, data: pd.DataFrame, value_col: str, date_col: str) -> Dict[str, float]:
        """Calculate trend analysis (slope, R²) for time series data"""
        if len(data) < 2:
            return {"slope": 0, "r_squared": 0, "trend_direction": "insufficient_data"}
        
        data_sorted = data.sort_values(date_col)
        x = np.arange(len(data_sorted))
        y = data_sorted[value_col].values
        
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
        
        trend_direction = "improving" if slope > 0 else "declining" if slope < 0 else "stable"
        
        return {
            "slope": slope,
            "r_squared": r_value ** 2,
            "trend_direction": trend_direction,
            "significance": "significant" if p_value < 0.05 else "not_significant"
        }
    
    def _calculate_nps(self, ratings: pd.Series) -> float:
        """Calculate Net Promoter Score from satisfaction ratings (1-5 scale)"""
        if len(ratings) == 0:
            return 0
        
        # Convert 1-5 ratings to NPS categories
        # 5 = Promoter, 4 = Passive, 1-3 = Detractor
        promoters = (ratings == 5).sum()
        detractors = (ratings <= 3).sum()
        total = len(ratings)
        
        nps = ((promoters - detractors) / total) * 100
        return nps
    
    async def _query_database(self, query: str) -> pd.DataFrame:
        """Execute database query and return DataFrame"""
        try:
            cursor = self.db_connection.cursor()
            cursor.execute(query)
            result = cursor.fetchall()
            columns = [desc[0] for desc in cursor.description]
            return pd.DataFrame(result, columns=columns)
        except Exception as e:
            self.logger.error(f"Database query failed: {e}")
            return pd.DataFrame()  # Return empty DataFrame on error

# ============================================================================
# REAL-TIME MONITORING & ALERTING
# ============================================================================

class TelosMonitoringSystem:
    """Real-time monitoring system with intelligent alerting"""
    
    def __init__(self, metrics_calculator: TelosMetricsCalculator, alert_thresholds: Dict[str, Any]):
        self.calculator = metrics_calculator
        self.thresholds = alert_thresholds
        self.logger = logging.getLogger(__name__)
        self.active_alerts = {}
        
    async def run_continuous_monitoring(self, check_interval_minutes: int = 15):
        """Run continuous monitoring with intelligent alerting"""
        while True:
            try:
                await self._perform_health_check()
                await asyncio.sleep(check_interval_minutes * 60)
            except Exception as e:
                self.logger.error(f"Monitoring cycle failed: {e}")
                await asyncio.sleep(60)  # Wait 1 minute before retry
    
    async def _perform_health_check(self):
        """Perform comprehensive system health check"""
        current_time = datetime.now()
        
        # Check critical metrics
        health_status = await self._check_critical_metrics()
        
        # Generate alerts for threshold violations
        new_alerts = self._evaluate_alert_conditions(health_status)
        
        # Process new alerts (send notifications, log, etc.)
        for alert in new_alerts:
            await self._process_alert(alert)
        
        # Update active alerts
        self._update_active_alerts(new_alerts)
        
        self.logger.info(f"Health check completed at {current_time}. Active alerts: {len(self.active_alerts)}")
    
    async def _check_critical_metrics(self) -> Dict[str, Any]:
        """Check critical system metrics for immediate issues"""
        today = date.today()
        yesterday = today - timedelta(days=1)
        
        # System availability check
        availability_query = """
            SELECT 
                COUNT(*) as total_checks,
                SUM(CASE WHEN status = 'UP' THEN 1 ELSE 0 END) as successful_checks
            FROM system_health_checks 
            WHERE check_time >=