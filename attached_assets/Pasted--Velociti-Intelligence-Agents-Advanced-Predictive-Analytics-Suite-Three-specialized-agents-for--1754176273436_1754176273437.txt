"""
Velociti Intelligence Agents - Advanced Predictive Analytics Suite
Three specialized agents for proactive revenue management intelligence

Agent 1: Surge Event Detector - External event correlation with demand patterns
Agent 2: Advance Booking Curve Alerting - Predictive booking pace intelligence  
Agent 3: Elasticity Change Alert - Fundamental demand shift detection

Think of this as the "mission control center" for airline revenue intelligence.
"""

import asyncio
import aiohttp
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, date
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import json
import logging
from scipy import stats, signal
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
import requests
from textblob import TextBlob
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# CORE AGENT FRAMEWORK
# ============================================================================

class AlertPriority(str, Enum):
    CRITICAL = "CRITICAL"
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"

class AgentStatus(str, Enum):
    ACTIVE = "ACTIVE"
    PROCESSING = "PROCESSING"
    SLEEPING = "SLEEPING"
    ERROR = "ERROR"

@dataclass
class IntelligenceAlert:
    """Standardized alert format across all Velociti agents"""
    agent_name: str
    alert_id: str
    priority: AlertPriority
    title: str
    description: str
    recommendation: str
    confidence_score: float
    revenue_impact_eur: int
    time_to_act: str
    affected_routes: List[str]
    supporting_data: Dict[str, Any]
    created_at: datetime
    expires_at: Optional[datetime] = None

@dataclass
class AgentContext:
    """Shared context across all agents"""
    airline_code: str = "EZY"
    base_currency: str = "EUR"
    operating_region: str = "Europe"
    analyst_timezone: str = "Europe/London"
    risk_tolerance: str = "moderate"  # conservative, moderate, aggressive
    revenue_targets: Dict[str, float] = None

# ============================================================================
# AGENT 1: SURGE EVENT DETECTOR
# ============================================================================

class SurgeEventDetector:
    """
    Monitors external events and correlates with historical demand patterns
    to predict revenue opportunities before competitors notice.
    
    Data Sources: Event APIs, Social Media, News Feeds, Historical Correlations
    """
    
    def __init__(self, context: AgentContext):
        self.context = context
        self.status = AgentStatus.ACTIVE
        self.logger = logging.getLogger(f"{__name__}.SurgeEventDetector")
        
        # Event correlation models trained on historical data
        self.event_impact_models = self._load_event_models()
        self.social_sentiment_threshold = 0.6
        self.confidence_threshold = 0.75
        
    def _load_event_models(self) -> Dict[str, Any]:
        """Load pre-trained models for event impact correlation"""
        return {
            "concerts": {
                "demand_multiplier": 1.35,
                "price_elasticity_change": -0.15,
                "peak_booking_window": 72,  # hours after announcement
                "duration_impact": 14  # days of elevated demand
            },
            "sports": {
                "demand_multiplier": 1.28,
                "price_elasticity_change": -0.12,
                "peak_booking_window": 48,
                "duration_impact": 10
            },
            "festivals": {
                "demand_multiplier": 1.45,
                "price_elasticity_change": -0.20,
                "peak_booking_window": 96,
                "duration_impact": 21
            },
            "conferences": {
                "demand_multiplier": 1.15,
                "price_elasticity_change": -0.08,
                "peak_booking_window": 168,  # 1 week
                "duration_impact": 7
            }
        }
    
    async def detect_surge_events(self, routes: List[str]) -> List[IntelligenceAlert]:
        """Main detection engine for surge events"""
        alerts = []
        
        try:
            # Step 1: Scan for new events across all route destinations
            new_events = await self._scan_event_sources(routes)
            
            # Step 2: Analyze social media sentiment and buzz
            social_signals = await self._analyze_social_sentiment(routes)
            
            # Step 3: Correlate with historical demand patterns
            for event in new_events:
                historical_impact = await self._calculate_historical_impact(event)
                
                # Step 4: Generate predictive alerts
                if historical_impact["confidence"] >= self.confidence_threshold:
                    alert = await self._create_surge_alert(event, historical_impact, social_signals)
                    alerts.append(alert)
            
            # Step 5: Detect viral/trending destinations without formal events
            viral_alerts = await self._detect_viral_destinations(routes, social_signals)
            alerts.extend(viral_alerts)
            
        except Exception as e:
            self.logger.error(f"Surge event detection failed: {e}")
            self.status = AgentStatus.ERROR
        
        return sorted(alerts, key=lambda x: x.confidence_score, reverse=True)
    
    async def _scan_event_sources(self, routes: List[str]) -> List[Dict[str, Any]]:
        """Scan multiple event data sources for new announcements"""
        events = []
        
        # Simulate event data - in production, integrate with:
        # - Eventbrite API
        # - Ticketmaster API  
        # - Local event calendars
        # - News feeds with NLP extraction
        
        simulated_events = [
            {
                "event_id": "bcn_primavera_2025",
                "event_type": "festival",
                "event_name": "Primavera Sound Barcelona 2025",
                "destination": "BCN",
                "announcement_date": datetime.now() - timedelta(hours=6),
                "event_dates": [datetime.now() + timedelta(days=45), datetime.now() + timedelta(days=47)],
                "expected_attendance": 200000,
                "venue": "Parc del Fòrum",
                "source": "official_announcement",
                "social_mentions": 15420,
                "news_coverage_score": 8.5
            },
            {
                "event_id": "mad_champions_final",
                "event_type": "sports",
                "event_name": "Champions League Final 2025",
                "destination": "MAD",
                "announcement_date": datetime.now() - timedelta(hours=2),
                "event_dates": [datetime.now() + timedelta(days=28)],
                "expected_attendance": 75000,
                "venue": "Santiago Bernabéu",
                "source": "uefa_official",
                "social_mentions": 89340,
                "news_coverage_score": 9.8
            }
        ]
        
        # Filter events relevant to our routes
        for event in simulated_events:
            relevant_routes = [r for r in routes if event["destination"] in r]
            if relevant_routes:
                event["affected_routes"] = relevant_routes
                events.append(event)
        
        return events
    
    async def _analyze_social_sentiment(self, routes: List[str]) -> Dict[str, Any]:
        """Analyze social media sentiment and travel intent signals"""
        
        # Simulate social sentiment analysis - in production, integrate with:
        # - Twitter API v2
        # - Instagram Graph API
        # - TikTok Research API
        # - Google Trends API
        
        sentiment_data = {}
        
        for route in routes:
            destination = route.split("-")[1] if "-" in route else route
            
            # Simulate sentiment analysis
            sentiment_data[destination] = {
                "mention_volume_24h": np.random.randint(500, 3000),
                "mention_growth_pct": np.random.uniform(-20, 150),
                "sentiment_score": np.random.uniform(0.3, 0.9),
                "travel_intent_score": np.random.uniform(0.4, 0.8),
                "trending_hashtags": [f"#{destination}travel", f"visit{destination}", f"{destination}2025"],
                "influencer_mentions": np.random.randint(0, 25),
                "viral_content_detected": np.random.choice([True, False], p=[0.15, 0.85])
            }
        
        return sentiment_data
    
    async def _calculate_historical_impact(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate predicted impact based on historical event correlations"""
        
        event_type = event["event_type"]
        model = self.event_impact_models.get(event_type, self.event_impact_models["conferences"])
        
        # Base impact calculation
        base_demand_lift = model["demand_multiplier"] - 1.0  # 35% for concerts
        attendance_factor = min(event["expected_attendance"] / 50000, 3.0)  # Cap at 3x
        social_factor = min(event["social_mentions"] / 10000, 2.0)  # Cap at 2x
        
        # Calculate predicted impact
        predicted_demand_lift = base_demand_lift * attendance_factor * social_factor
        revenue_opportunity = predicted_demand_lift * 150000  # EUR base route revenue
        
        # Confidence calculation based on data quality
        confidence_factors = [
            min(event["news_coverage_score"] / 10, 1.0),  # News quality
            min(event["social_mentions"] / 20000, 1.0),   # Social buzz
            0.9 if event["source"] == "official_announcement" else 0.7,  # Source reliability
            0.8  # Historical model accuracy
        ]
        
        confidence = np.mean(confidence_factors)
        
        return {
            "predicted_demand_lift_pct": predicted_demand_lift * 100,
            "revenue_opportunity_eur": int(revenue_opportunity),
            "optimal_pricing_window": model["peak_booking_window"],
            "impact_duration_days": model["duration_impact"],
            "confidence": confidence,
            "risk_factors": self._identify_risk_factors(event)
        }
    
    def _identify_risk_factors(self, event: Dict[str, Any]) -> List[str]:
        """Identify potential risks that could reduce event impact"""
        risks = []
        
        if event["event_dates"][0] - datetime.now() < timedelta(days=14):
            risks.append("Short booking window may limit capture")
        
        if event["event_type"] == "festival" and datetime.now().month in [11, 12, 1, 2]:
            risks.append("Winter festival may have lower international draw")
        
        if event["expected_attendance"] > 100000:
            risks.append("Large event may create accommodation shortage")
        
        return risks
    
    async def _create_surge_alert(self, event: Dict[str, Any], 
                                impact: Dict[str, Any], 
                                social_signals: Dict[str, Any]) -> IntelligenceAlert:
        """Create standardized alert for surge event detection"""
        
        destination = event["destination"]
        affected_routes = event["affected_routes"]
        
        # Priority based on revenue impact and timing
        if impact["revenue_opportunity_eur"] > 100000 and impact["confidence"] > 0.85:
            priority = AlertPriority.CRITICAL
        elif impact["revenue_opportunity_eur"] > 50000 and impact["confidence"] > 0.75:
            priority = AlertPriority.HIGH
        else:
            priority = AlertPriority.MEDIUM
        
        # Time sensitivity calculation
        optimal_window_hours = impact["optimal_pricing_window"]
        if optimal_window_hours <= 24:
            time_to_act = "IMMEDIATE (within 4 hours)"
        elif optimal_window_hours <= 72:
            time_to_act = "TODAY"
        else:
            time_to_act = "THIS WEEK"
        
        # Build recommendation
        demand_lift = impact["predicted_demand_lift_pct"]
        price_increase = min(demand_lift * 0.6, 25)  # Conservative pricing
        
        recommendation = f"""
        REVENUE OPPORTUNITY DETECTED:
        
        1. IMMEDIATE ACTIONS (next {optimal_window_hours} hours):
           • Increase prices {price_increase:.0f}% for flights departing during event period
           • Monitor competitor response - they may not have detected this yet
           • Set up hourly booking pace alerts
        
        2. TACTICAL CONSIDERATIONS:
           • Event duration: {impact['impact_duration_days']} days of elevated demand
           • Social buzz momentum: {social_signals.get(destination, {}).get('mention_growth_pct', 0):.0f}% increase
           • Risk factors: {', '.join(impact['risk_factors']) if impact['risk_factors'] else 'None identified'}
        
        3. COMPETITIVE INTELLIGENCE:
           • Monitor if competitors adjust pricing in next 24 hours
           • If no competitive response, consider additional 5-8% increase
        """
        
        return IntelligenceAlert(
            agent_name="SurgeEventDetector",
            alert_id=f"surge_{event['event_id']}_{datetime.now().strftime('%Y%m%d_%H%M')}",
            priority=priority,
            title=f"{event['event_name']} - Demand Surge Opportunity",
            description=f"{event['event_name']} announced {(datetime.now() - event['announcement_date']).seconds // 3600} hours ago. Historical analysis predicts {demand_lift:.0f}% demand increase for {', '.join(affected_routes)}. Social media buzz growing at {social_signals.get(destination, {}).get('mention_growth_pct', 0):.0f}% rate.",
            recommendation=recommendation.strip(),
            confidence_score=impact["confidence"],
            revenue_impact_eur=impact["revenue_opportunity_eur"],
            time_to_act=time_to_act,
            affected_routes=affected_routes,
            supporting_data={
                "event_details": event,
                "impact_analysis": impact,
                "social_signals": social_signals.get(destination, {}),
                "historical_comparisons": f"Similar {event['event_type']} events averaged {self.event_impact_models[event['event_type']]['demand_multiplier']:.0%} demand lift"
            },
            created_at=datetime.now()
        )
    
    async def _detect_viral_destinations(self, routes: List[str], 
                                       social_signals: Dict[str, Any]) -> List[IntelligenceAlert]:
        """Detect viral travel destinations without formal events"""
        viral_alerts = []
        
        for route in routes:
            destination = route.split("-")[1] if "-" in route else route
            signals = social_signals.get(destination, {})
            
            # Viral detection criteria
            mention_growth = signals.get("mention_growth_pct", 0)
            viral_content = signals.get("viral_content_detected", False)
            influencer_mentions = signals.get("influencer_mentions", 0)
            
            if (mention_growth > 100 and viral_content) or influencer_mentions > 15:
                # This destination is going viral
                revenue_impact = int(mention_growth * 1000)  # Simplified calculation
                
                alert = IntelligenceAlert(
                    agent_name="SurgeEventDetector",
                    alert_id=f"viral_{destination}_{datetime.now().strftime('%Y%m%d_%H%M')}",
                    priority=AlertPriority.HIGH if mention_growth > 150 else AlertPriority.MEDIUM,
                    title=f"Viral Travel Trend Detected: {destination}",
                    description=f"{destination} experiencing viral social media growth (+{mention_growth:.0f}% mentions, {influencer_mentions} influencer posts). No specific event identified - likely organic travel content trend.",
                    recommendation=f"""
                    VIRAL OPPORTUNITY RESPONSE:
                    
                    1. IMMEDIATE (next 24 hours):
                       • Test 8-12% price increase on {route} flights
                       • Monitor conversion rates hourly
                       • Track competitor pricing response
                    
                    2. CONTENT STRATEGY:
                       • Investigate viral content source (TikTok trend, Instagram campaign?)
                       • Consider amplifying trend with targeted social ads
                       • Partner with travel influencers if budget allows
                    
                    3. CAPACITY PLANNING:
                       • If demand surge validates, consider aircraft swap to larger gauge
                       • Monitor booking pace vs. viral content timeline
                    """,
                    confidence_score=0.70,  # Lower confidence for viral trends
                    revenue_impact_eur=revenue_impact,
                    time_to_act="TODAY",
                    affected_routes=[route],
                    supporting_data={
                        "viral_metrics": signals,
                        "trend_analysis": "Organic social media trend - no specific event catalyst"
                    },
                    created_at=datetime.now(),
                    expires_at=datetime.now() + timedelta(days=7)  # Viral trends are time-sensitive
                )
                
                viral_alerts.append(alert)
        
        return viral_alerts

# ============================================================================
# AGENT 2: ADVANCE BOOKING CURVE ALERTING
# ============================================================================

class AdvanceBookingCurveAlerting:
    """
    Analyzes booking patterns and predicts optimal intervention points
    before traditional revenue management systems detect anomalies.
    
    Predictive Intelligence: "LGW-BCN booking 15% ahead of curve - pricing window opens in 48 hours"
    """
    
    def __init__(self, context: AgentContext):
        self.context = context
        self.status = AgentStatus.ACTIVE
        self.logger = logging.getLogger(f"{__name__}.AdvanceBookingCurveAlerting")
        
        # ML models for booking curve prediction
        self.curve_models = self._initialize_prediction_models()
        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
        self.confidence_threshold = 0.80
        
    def _initialize_prediction_models(self) -> Dict[str, Any]:
        """Initialize booking curve prediction models"""
        return {
            "seasonal_patterns": {
                "summer": {"peak_booking_days": [45, 30, 14], "acceleration_factor": 1.2},
                "winter": {"peak_booking_days": [60, 35, 21], "acceleration_factor": 0.9},
                "shoulder": {"peak_booking_days": [50, 28, 14], "acceleration_factor": 1.0}
            },
            "route_characteristics": {
                "leisure": {"booking_window": 45, "volatility": 0.3, "price_sensitivity": 0.8},
                "business": {"booking_window": 21, "volatility": 0.2, "price_sensitivity": 0.4},
                "mixed": {"booking_window": 35, "volatility": 0.25, "price_sensitivity": 0.6}
            },
            "day_of_week_patterns": {
                "monday": 0.7, "tuesday": 0.6, "wednesday": 0.6, "thursday": 0.8,
                "friday": 1.3, "saturday": 1.1, "sunday": 1.4
            }
        }
    
    async def analyze_booking_curves(self, routes: List[str]) -> List[IntelligenceAlert]:
        """Main analysis engine for booking curve anomalies and opportunities"""
        alerts = []
        
        try:
            for route in routes:
                # Step 1: Get current booking data
                booking_data = await self._get_booking_curve_data(route)
                
                # Step 2: Generate expected curve based on historical patterns
                expected_curve = await self._generate_expected_curve(route, booking_data)
                
                # Step 3: Detect anomalies and opportunities
                anomalies = await self._detect_curve_anomalies(route, booking_data, expected_curve)
                
                # Step 4: Predict optimal intervention points
                interventions = await self._predict_intervention_points(route, booking_data, expected_curve)
                
                # Step 5: Generate alerts
                for anomaly in anomalies:
                    alert = await self._create_booking_alert(route, anomaly, interventions)
                    alerts.append(alert)
        
        except Exception as e:
            self.logger.error(f"Booking curve analysis failed: {e}")
            self.status = AgentStatus.ERROR
        
        return sorted(alerts, key=lambda x: x.confidence_score, reverse=True)
    
    async def _get_booking_curve_data(self, route: str) -> Dict[str, Any]:
        """Retrieve current booking data for route analysis"""
        
        # Simulate booking curve data - in production, query from:
        # - Flight performance tables
        # - Real-time booking systems
        # - Historical curve database
        
        days_out = list(range(1, 91))  # 90 days out
        
        # Generate realistic booking curve with patterns
        base_curve = self._generate_base_curve(route, days_out)
        
        # Add current anomalies for testing
        if "BCN" in route:
            # Barcelona showing early booking surge
            base_curve = [b * 1.15 if d > 30 else b for d, b in zip(days_out, base_curve)]
        elif "PMI" in route:
            # Palma showing slower than expected pace
            base_curve = [b * 0.92 if 14 < d < 45 else b for d, b in zip(days_out, base_curve)]
        
        return {
            "route": route,
            "days_out": days_out,
            "bookings_by_day": base_curve,
            "current_load_factor": np.random.uniform(0.65, 0.95),
            "forecast_load_factor": np.random.uniform(0.70, 0.92),
            "price_points": [80 + d * 0.5 + np.random.normal(0, 5) for d in days_out],
            "last_updated": datetime.now()
        }
    
    def _generate_base_curve(self, route: str, days_out: List[int]) -> List[float]:
        """Generate realistic booking curve based on route characteristics"""
        
        # Determine route characteristics
        if any(dest in route for dest in ["PMI", "AGP", "LPA"]):  # Leisure destinations
            route_type = "leisure"
        elif any(dest in route for dest in ["CDG", "AMS", "FRA"]):  # Business destinations
            route_type = "business" 
        else:
            route_type = "mixed"
        
        characteristics = self.curve_models["route_characteristics"][route_type]
        
        # Generate S-curve with noise
        curve = []
        for day in days_out:
            # Sigmoid function for booking curve
            x = (characteristics["booking_window"] - day) / 10
            base_booking_rate = 1 / (1 + np.exp(-x))
            
            # Add seasonal and day-of-week effects
            seasonal_factor = self._get_seasonal_factor()
            noise = np.random.normal(1, characteristics["volatility"] * 0.1)
            
            booking_rate = base_booking_rate * seasonal_factor * noise
            curve.append(max(0, booking_rate))
        
        return curve
    
    def _get_seasonal_factor(self) -> float:
        """Get current seasonal adjustment factor"""
        month = datetime.now().month
        if month in [6, 7, 8]:  # Summer
            return self.curve_models["seasonal_patterns"]["summer"]["acceleration_factor"]
        elif month in [12, 1, 2]:  # Winter
            return self.curve_models["seasonal_patterns"]["winter"]["acceleration_factor"]
        else:  # Shoulder
            return self.curve_models["seasonal_patterns"]["shoulder"]["acceleration_factor"]
    
    async def _generate_expected_curve(self, route: str, booking_data: Dict[str, Any]) -> Dict[str, Any]:
        """Generate expected booking curve based on historical patterns"""
        
        # In production, this would use ML models trained on historical data
        # For demo, we'll create expected patterns with slight variations
        
        expected_bookings = []
        actual_bookings = booking_data["bookings_by_day"]
        
        for i, actual in enumerate(actual_bookings):
            # Expected value with some historical noise
            expected = actual * np.random.uniform(0.95, 1.05)
            expected_bookings.append(expected)
        
        return {
            "route": route,
            "expected_bookings": expected_bookings,
            "confidence_interval_upper": [e * 1.1 for e in expected_bookings],
            "confidence_interval_lower": [e * 0.9 for e in expected_bookings],
            "model_accuracy": 0.87,
            "historical_data_points": 365
        }
    
    async def _detect_curve_anomalies(self, route: str, 
                                    booking_data: Dict[str, Any], 
                                    expected_curve: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detect significant deviations from expected booking patterns"""
        
        anomalies = []
        actual = np.array(booking_data["bookings_by_day"])
        expected = np.array(expected_curve["expected_bookings"])
        
        # Calculate percentage deviation
        deviation = (actual - expected) / expected * 100
        
        # Detect significant anomalies (>15% deviation sustained over 5+ days)
        for i in range(len(deviation) - 5):
            window = deviation[i:i+5]
            if np.mean(np.abs(window)) > 15:  # 15% sustained deviation
                
                anomaly_type = "AHEAD_OF_CURVE" if np.mean(window) > 0 else "BEHIND_CURVE"
                severity = "CRITICAL" if np.mean(np.abs(window)) > 25 else "HIGH" if np.mean(np.abs(window)) > 20 else "MEDIUM"
                
                anomalies.append({
                    "route": route,
                    "anomaly_type": anomaly_type,
                    "severity": severity,
                    "start_day": booking_data["days_out"][i],
                    "duration_days": 5,
                    "avg_deviation_pct": np.mean(window),
                    "peak_deviation_pct": np.max(np.abs(window)),
                    "confidence": min(0.95, expected_curve["model_accuracy"] + 0.05),
                    "booking_window": f"{booking_data['days_out'][i]}-{booking_data['days_out'][i+4]} days out"
                })
        
        return anomalies
    
    async def _predict_intervention_points(self, route: str,
                                         booking_data: Dict[str, Any], 
                                         expected_curve: Dict[str, Any]) -> Dict[str, Any]:
        """Predict optimal timing for pricing interventions"""
        
        # Analyze booking acceleration/deceleration
        actual = np.array(booking_data["bookings_by_day"])
        
        # Calculate booking velocity (rate of change)
        velocity = np.gradient(actual)
        acceleration = np.gradient(velocity)
        
        # Identify optimal intervention windows
        intervention_points = []
        
        for i in range(10, len(actual) - 10):  # Avoid edge effects
            days_out = booking_data["days_out"][i]
            
            # Look for acceleration inflection points
            if days_out in [45, 30, 21, 14, 7]:  # Key booking windows
                current_velocity = velocity[i]
                trend = "accelerating" if acceleration[i] > 0 else "decelerating"
                
                # Predict optimal intervention timing
                if trend == "accelerating" and current_velocity > np.mean(velocity):
                    intervention_type = "PRICE_INCREASE_OPPORTUNITY"
                    urgency = "HIGH" if days_out <= 21 else "MEDIUM"
                elif trend == "decelerating" and current_velocity < np.mean(velocity):
                    intervention_type = "DEMAND_STIMULATION_NEEDED"
                    urgency = "CRITICAL" if days_out <= 14 else "HIGH"
                else:
                    continue
                
                intervention_points.append({
                    "days_out": days_out,
                    "intervention_type": intervention_type,
                    "urgency": urgency,
                    "booking_velocity": current_velocity,
                    "predicted_window": f"{days_out-3} to {days_out+3} days out",
                    "confidence": 0.78
                })
        
        return {
            "route": route,
            "intervention_points": intervention_points,
            "overall_trend": "positive" if np.mean(velocity) > 0 else "negative",
            "volatility_score": np.std(velocity)
        }
    
    async def _create_booking_alert(self, route: str, 
                                  anomaly: Dict[str, Any], 
                                  interventions: Dict[str, Any]) -> IntelligenceAlert:
        """Create standardized alert for booking curve anomalies"""
        
        # Determine priority based on anomaly severity and revenue impact
        severity = anomaly["severity"]
        deviation = abs(anomaly["avg_deviation_pct"])
        
        if severity == "CRITICAL":
            priority = AlertPriority.CRITICAL
            revenue_impact = int(deviation * 5000)  # High impact calculation
        elif severity == "HIGH":
            priority = AlertPriority.HIGH
            revenue_impact = int(deviation * 3000)
        else:
            priority = AlertPriority.MEDIUM
            revenue_impact = int(deviation * 1500)
        
        # Build contextual description
        trend_direction = "ahead of" if anomaly["anomaly_type"] == "AHEAD_OF_CURVE" else "behind"
        
        description = f"""
        {route} booking pace is {deviation:.1f}% {trend_direction} expected curve for flights departing {anomaly['booking_window']}.
        
        Pattern Analysis:
        • Deviation detected: {anomaly['avg_deviation_pct']:+.1f}% vs historical pattern
        • Duration: {anomaly['duration_days']} days sustained
        • Confidence: {anomaly['confidence']:.0%} (based on {365} days historical data)
        
        Market Context:
        • Overall trend: {interventions['overall_trend']} booking velocity
        • Route volatility: {'High' if interventions['volatility_score'] > 0.5 else 'Normal'}
        """.strip()
        
        # Build tactical recommendations
        if anomaly["anomaly_type"] == "AHEAD_OF_CURVE":
            recommendation = f"""
            BOOKING PACE AHEAD - REVENUE CAPTURE OPPORTUNITY:
            
            1. IMMEDIATE ACTIONS (next 24-48 hours):
               • Test price increase of 8-12% for affected booking window
               • Monitor conversion rates hourly during test period
               • Set up competitor price monitoring for this route
            
            2. STRATEGIC POSITIONING:
               • Strong demand validates premium positioning
               • Consider closing lower fare classes earlier than planned
               • Evaluate aircraft swap to larger gauge if load factor trending >90%
            
            3. RISK MITIGATION:
               • Monitor for external demand drivers (events, competitor issues)
               • Set booking pace alerts if trend reverses suddenly
               • Prepare promotional backup if overcorrection occurs
            
            OPTIMAL ACTION WINDOW: Next {48 if anomaly['severity'] == 'CRITICAL' else 72} hours
            """
        else:  # BEHIND_CURVE
            recommendation = f"""
            BOOKING PACE LAGGING - DEMAND STIMULATION REQUIRED:
            
            1. IMMEDIATE ACTIONS (next 12-24 hours):
               • Investigate root cause: competitive pressure, capacity increase, external factors
               • Consider tactical promotion: 10-15% discount for limited time
               • Review fare class availability - ensure competitive positioning
            
            2. DIAGNOSTIC ANALYSIS:
               • Check competitor pricing changes in past 7 days
               • Verify no operational issues affecting route perception
               • Analyze search volume trends for destination
            
            3. RECOVERY STRATEGY:
               • If 20+ days out: Promotional pricing with limited inventory
               • If <20 days out: Focus on operational excellence and ancillary revenue
               • Monitor daily booking pace for early recovery signs
            
            URGENCY LEVEL: {'CRITICAL - Act within 12 hours' if anomaly['severity'] == 'CRITICAL' else 'HIGH - Act within 24 hours'}
            """
        
        # Time sensitivity
        days_out_avg = (anomaly["start_day"] + anomaly["start_day"] - anomaly["duration_days"]) / 2
        if days_out_avg <= 14:
            time_to_act = "IMMEDIATE (within 12 hours)"
        elif days_out_avg <= 30:
            time_to_act = "TODAY"
        else:
            time_to_act = "THIS WEEK"
        
        return IntelligenceAlert(
            agent_name="AdvanceBookingCurveAlerting",
            alert_id=f"booking_{route.replace('-', '')}_{anomaly['anomaly_type']}_{datetime.now().strftime('%Y%m%d_%H%M')}",
            priority=priority,
            title=f"Booking Curve Anomaly: {route} {deviation:.0f}% {trend_direction.title()} Pace",
            description=description,
            recommendation=recommendation.strip(),
            confidence_score=anomaly["confidence"],
            revenue_impact_eur=revenue_impact,
            time_to_act=time_to_act,
            affected_routes=[route],
            supporting_data={
                "anomaly_details": anomaly,
                "intervention_analysis": interventions,
                "booking_window": anomaly["booking_window"],
                "model_performance": f"Historical accuracy: {anomaly['confidence']:.0%}"
            },
            created_at=datetime.now()
        )

# ============================================================================
# AGENT 3: ELASTICITY CHANGE ALERT
# ============================================================================

class ElasticityChangeAlert:
    """
    Detects fundamental shifts in price sensitivity that indicate 
    changing market dynamics and require strategic pricing adjustments.
    
    Advanced Intelligence: "LGW-PMI price sensitivity dropped 30% - premium capture window"
    """
    
    def __init__(self, context: AgentContext):
        self.context = context
        self.status = AgentStatus.ACTIVE
        self.logger = logging.getLogger(f"{__name__}.ElasticityChangeAlert")
        
        # Elasticity detection models
        self.elasticity_models = self._initialize_elasticity_models()
        self.significance_threshold = 0.15  # 15% change in elasticity
        self.confidence_threshold = 0.75
        
    def _initialize_elasticity_models(self) -> Dict[str, Any]:
        """Initialize price elasticity detection models"""
        return {
            "baseline_elasticity": {
                "leisure_routes": -1.2,  # More price sensitive
                "business_routes": -0.6,  # Less price sensitive
                "mixed_routes": -0.9     # Moderate sensitivity
            },
            "seasonal_adjustments": {
                "peak_season": 0.8,    # Less elastic during peak
                "shoulder_season": 1.0,  # Normal elasticity
                "low_season": 1.3      # More elastic during low season
            },
            "market_factors": {
                "competitor_entry": 1.4,      # More elastic with competition
                "capacity_constraint": 0.7,   # Less elastic when constrained
                "economic_stress": 1.5,       # More elastic during downturns
                "event_driven": 0.6          # Less elastic for events
            }
        }
    
    async def detect_elasticity_changes(self, routes: List[str]) -> List[IntelligenceAlert]:
        """Main detection engine for price elasticity shifts"""
        alerts = []
        
        try:
            for route in routes:
                # Step 1: Calculate current price elasticity
                current_elasticity = await self._calculate_current_elasticity(route)
                
                # Step 2: Compare against historical baseline
                baseline_elasticity = await self._get_baseline_elasticity(route)
                
                # Step 3: Detect significant changes
                elasticity_change = await self._analyze_elasticity_shift(
                    route, current_elasticity, baseline_elasticity
                )
                
                # Step 4: Generate alerts for significant changes
                if elasticity_change["significance"] >= self.significance_threshold:
                    alert = await self._create_elasticity_alert(route, elasticity_change)
                    alerts.append(alert)
        
        except Exception as e:
            self.logger.error(f"Elasticity change detection failed: {e}")
            self.status = AgentStatus.ERROR
        
        return sorted(alerts, key=lambda x: x.confidence_score, reverse=True)
    
    async def _calculate_current_elasticity(self, route: str) -> Dict[str, Any]:
        """Calculate current price elasticity using recent booking/pricing data"""
        
        # Simulate elasticity calculation - in production, use:
        # - Recent price changes and booking responses
        # - A/B testing results
        # - Econometric analysis of price-demand relationship
        
        # Generate realistic elasticity data with market factors
        base_elasticity = -0.9  # Typical airline elasticity
        
        # Add market-specific adjustments
        if any(dest in route for dest in ["PMI", "AGP", "IBZ"]):  # Leisure destinations
            base_elasticity = -1.2
            market_type = "leisure"
        elif any(dest in route for dest in ["CDG", "AMS", "FRA"]):  # Business destinations
            base_elasticity = -0.6
            market_type = "business"
        else:
            base_elasticity = -0.9
            market_type = "mixed"
        
        # Simulate recent elasticity with some variance
        recent_elasticity = base_elasticity * np.random.uniform(0.7, 1.4)
        
        # Add specific scenarios for testing
        if "PMI" in route:
            recent_elasticity = -0.85  # Less elastic than expected (premium opportunity)
        elif "BCN" in route:
            recent_elasticity = -1.45  # More elastic than expected (price pressure)
        
        return {
            "route": route,
            "current_elasticity": recent_elasticity,
            "market_type": market_type,
            "measurement_period": "last_14_days",
            "data_points": np.random.randint(45, 125),
            "r_squared": np.random.uniform(0.65, 0.92),
            "confidence_interval": [recent_elasticity * 0.9, recent_elasticity * 1.1],
            "calculation_method": "log_log_regression",
            "last_calculated": datetime.now()
        }
    
    async def _get_baseline_elasticity(self, route: str) -> Dict[str, Any]:
        """Get historical baseline elasticity for comparison"""
        
        # Simulate historical baseline - in production, query from:
        # - Historical elasticity database
        # - Industry benchmarks
        # - Route-specific models
        
        # Determine route characteristics for baseline
        if any(dest in route for dest in ["PMI", "AGP", "IBZ"]):
            baseline = self.elasticity_models["baseline_elasticity"]["leisure_routes"]
            route_type = "leisure"
        elif any(dest in route for dest in ["CDG", "AMS", "FRA"]):
            baseline = self.elasticity_models["baseline_elasticity"]["business_routes"]
            route_type = "business"
        else:
            baseline = self.elasticity_models["baseline_elasticity"]["mixed_routes"]
            route_type = "mixed"
        
        # Apply seasonal adjustment
        seasonal_factor = self._get_seasonal_elasticity_factor()
        adjusted_baseline = baseline * seasonal_factor
        
        return {
            "route": route,
            "route_type": route_type,
            "baseline_elasticity": adjusted_baseline,
            "historical_period": "12_months",
            "seasonal_adjustment": seasonal_factor,
            "industry_benchmark": baseline,
            "confidence_level": 0.85,
            "last_updated": datetime.now() - timedelta(days=30)
        }
    
    def _get_seasonal_elasticity_factor(self) -> float:
        """Get current seasonal adjustment for elasticity"""
        month = datetime.now().month
        
        if month in [6, 7, 8]:  # Peak summer season
            return self.elasticity_models["seasonal_adjustments"]["peak_season"]
        elif month in [12, 1, 2]:  # Low winter season
            return self.elasticity_models["seasonal_adjustments"]["low_season"]
        else:  # Shoulder season
            return self.elasticity_models["seasonal_adjustments"]["shoulder_season"]
    
    async def _analyze_elasticity_shift(self, route: str, 
                                      current: Dict[str, Any], 
                                      baseline: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze the significance and implications of elasticity changes"""
        
        current_value = current["current_elasticity"]
        baseline_value = baseline["baseline_elasticity"]
        
        # Calculate percentage change
        elasticity_change_pct = ((current_value - baseline_value) / abs(baseline_value)) * 100
        
        # Determine change direction and magnitude
        if elasticity_change_pct > 15:  # Becoming more elastic (worse for pricing)
            change_direction = "MORE_ELASTIC"
            pricing_implication = "DEFENSIVE"
        elif elasticity_change_pct < -15:  # Becoming less elastic (better for pricing)
            change_direction = "LESS_ELASTIC"
            pricing_implication = "AGGRESSIVE"
        else:
            change_direction = "STABLE"
            pricing_implication = "MAINTAIN"
        
        # Calculate statistical significance
        statistical_confidence = min(0.95, (current["r_squared"] + baseline["confidence_level"]) / 2)
        
        # Identify potential drivers
        potential_drivers = self._identify_elasticity_drivers(route, change_direction, elasticity_change_pct)
        
        # Calculate revenue impact
        revenue_impact = self._estimate_revenue_impact(route, elasticity_change_pct, pricing_implication)
        
        return {
            "route": route,
            "current_elasticity": current_value,
            "baseline_elasticity": baseline_value,
            "change_percentage": elasticity_change_pct,
            "change_direction": change_direction,
            "pricing_implication": pricing_implication,
            "significance": abs(elasticity_change_pct) / 100,
            "statistical_confidence": statistical_confidence,
            "potential_drivers": potential_drivers,
            "revenue_impact_eur": revenue_impact,
            "measurement_quality": current["r_squared"]
        }
    
    def _identify_elasticity_drivers(self, route: str, direction: str, magnitude: float) -> List[str]:
        """Identify potential factors driving elasticity changes"""
        drivers = []
        
        # Market structure drivers
        if direction == "MORE_ELASTIC":
            drivers.extend([
                "Increased competitive pressure",
                "New market entrants",
                "Economic uncertainty affecting travel spending",
                "Capacity increases in market"
            ])
        else:  # LESS_ELASTIC
            drivers.extend([
                "Market consolidation reducing options",
                "Unique market positioning advantage",
                "Strong brand loyalty development",
                "Event-driven demand surge"
            ])
        
        # Route-specific factors
        if any(dest in route for dest in ["PMI", "AGP", "IBZ"]):
            if direction == "LESS_ELASTIC":
                drivers.append("Leisure destination gaining exclusivity appeal")
            else:
                drivers.append("Increased leisure travel competition")
        
        # Magnitude-specific drivers
        if magnitude > 25:
            drivers.append("Fundamental market shift detected")
            drivers.append("Possible external shock affecting demand")
        
        return drivers[:4]  # Return top 4 most likely drivers
    
    def _estimate_revenue_impact(self, route: str, elasticity_change: float, 
                               pricing_implication: str) -> int:
        """Estimate revenue impact of elasticity change"""
        
        # Base route revenue estimate (simplified)
        base_monthly_revenue = 200000  # EUR per route per month
        
        if pricing_implication == "AGGRESSIVE":
            # Less elastic = can raise prices more
            potential_price_increase = min(abs(elasticity_change) * 0.8, 20)  # Cap at 20%
            revenue_uplift = base_monthly_revenue * (potential_price_increase / 100)
        elif pricing_implication == "DEFENSIVE":
            # More elastic = need to be careful with pricing
            potential_revenue_risk = min(abs(elasticity_change) * 0.6, 15)  # Cap at 15%
            revenue_uplift = -base_monthly_revenue * (potential_revenue_risk / 100)
        else:
            revenue_uplift = 0
        
        return int(revenue_uplift)
    
    async def _create_elasticity_alert(self, route: str, 
                                     elasticity_change: Dict[str, Any]) -> IntelligenceAlert:
        """Create standardized alert for elasticity changes"""
        
        change_pct = elasticity_change["change_percentage"]
        direction = elasticity_change["change_direction"]
        implication = elasticity_change["pricing_implication"]
        
        # Determine priority based on magnitude and revenue impact
        if abs(change_pct) > 30 and elasticity_change["statistical_confidence"] > 0.85:
            priority = AlertPriority.CRITICAL
        elif abs(change_pct) > 20 and elasticity_change["statistical_confidence"] > 0.75:
            priority = AlertPriority.HIGH
        else:
            priority = AlertPriority.MEDIUM
        
        # Build descriptive title
        elasticity_direction = "Less Price Sensitive" if direction == "LESS_ELASTIC" else "More Price Sensitive"
        title = f"Market Elasticity Shift: {route} - {elasticity_direction} ({abs(change_pct):.0f}%)"
        
        # Build detailed description
        description = f"""
        Fundamental price sensitivity change detected for {route}:
        
        Elasticity Analysis:
        • Current elasticity: {elasticity_change['current_elasticity']:.2f}
        • Historical baseline: {elasticity_change['baseline_elasticity']:.2f}
        • Change magnitude: {change_pct:+.1f}% ({'less' if direction == 'LESS_ELASTIC' else 'more'} price sensitive)
        
        Statistical Confidence: {elasticity_change['statistical_confidence']:.0%}
        Market Implications: {implication} pricing strategy recommended
        
        Potential Drivers:
        {chr(10).join(f"• {driver}" for driver in elasticity_change['potential_drivers'])}
        """.strip()
        
        # Build strategic recommendations
        if implication == "AGGRESSIVE":
            recommendation = f"""
            REDUCED PRICE SENSITIVITY - PREMIUM CAPTURE OPPORTUNITY:
            
            1. IMMEDIATE TESTING (next 48 hours):
               • Test price increases of 8-12% on select flights
               • Monitor conversion rates and competitor response
               • Focus on peak travel periods first
            
            2. STRATEGIC POSITIONING:
               • Market conditions support premium positioning
               • Consider fare class mix optimization (more premium inventory)
               • Evaluate ancillary bundling opportunities
            
            3. COMPETITIVE INTELLIGENCE:
               • Monitor if competitors detect same elasticity shift
               • Prepare for potential competitive response
               • Document market conditions enabling premium pricing
            
            REVENUE OPPORTUNITY: Up to €{abs(elasticity_change['revenue_impact_eur']):,} monthly
            OPTIMAL ACTION WINDOW: Next 72 hours while market conditions persist
            """
        else:  # DEFENSIVE
            recommendation = f"""
            INCREASED PRICE SENSITIVITY - DEFENSIVE STRATEGY REQUIRED:
            
            1. IMMEDIATE RISK MITIGATION (next 24 hours):
               • Audit current pricing vs competitors
               • Identify potential promotional opportunities
               • Review fare class availability and restrictions
            
            2. MARKET RESPONSE STRATEGY:
               • Enhanced value messaging in marketing
               • Consider tactical promotions for price-sensitive segments
               • Focus on operational excellence to justify pricing
            
            3. COMPETITIVE POSITIONING:
               • Monitor market share trends closely
               • Prepare dynamic pricing responses
               • Evaluate route profitability under new conditions
            
            REVENUE AT RISK: Up to €{abs(elasticity_change['revenue_impact_eur']):,} monthly
            URGENCY LEVEL: HIGH - Market conditions requiring immediate attention
            """
        
        # Time sensitivity
        time_to_act = "IMMEDIATE (within 24 hours)" if priority == AlertPriority.CRITICAL else "TODAY"
        
        return IntelligenceAlert(
            agent_name="ElasticityChangeAlert",
            alert_id=f"elasticity_{route.replace('-', '')}_{direction}_{datetime.now().strftime('%Y%m%d_%H%M')}",
            priority=priority,
            title=title,
            description=description,
            recommendation=recommendation.strip(),
            confidence_score=elasticity_change["statistical_confidence"],
            revenue_impact_eur=abs(elasticity_change["revenue_impact_eur"]),
            time_to_act=time_to_act,
            affected_routes=[route],
            supporting_data={
                "elasticity_analysis": elasticity_change,
                "market_drivers": elasticity_change["potential_drivers"],
                "statistical_quality": f"R² = {elasticity_change['measurement_quality']:.2f}",
                "pricing_strategy": implication
            },
            created_at=datetime.now()
        )

# ============================================================================
# AGENT ORCHESTRATION ENGINE
# ============================================================================

class VelocitiAgentOrchestrator:
    """
    Orchestrates multiple intelligence agents and synthesizes insights
    for comprehensive morning briefing generation.
    """
    
    def __init__(self, context: AgentContext):
        self.context = context
        self.logger = logging.getLogger(f"{__name__}.VelocitiAgentOrchestrator")
        
        # Initialize agents
        self.surge_detector = SurgeEventDetector(context)
        self.booking_analyzer = AdvanceBookingCurveAlerting(context)
        self.elasticity_monitor = ElasticityChangeAlert(context)
        
        self.agents = {
            "surge_events": self.surge_detector,
            "booking_curves": self.booking_analyzer,
            "elasticity_changes": self.elasticity_monitor
        }
    
    async def run_night_shift_analysis(self, routes: List[str]) -> Dict[str, Any]:
        """Execute all agents and synthesize morning briefing"""
        
        self.logger.info(f"Starting NightShift analysis for {len(routes)} routes")
        start_time = datetime.now()
        
        try:
            # Run all agents in parallel
            agent_tasks = {
                "surge_events": self.surge_detector.detect_surge_events(routes),
                "booking_curves": self.booking_analyzer.analyze_booking_curves(routes),
                "elasticity_changes": self.elasticity_monitor.detect_elasticity_changes(routes)
            }
            
            results = await asyncio.gather(*agent_tasks.values(), return_exceptions=True)
            agent_results = dict(zip(agent_tasks.keys(), results))
            
            # Synthesize insights
            morning_briefing = await self._synthesize_briefing(agent_results, routes)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            self.logger.info(f"NightShift analysis completed in {processing_time:.2f} seconds")
            
            return {
                "status": "SUCCESS",
                "processing_time_seconds": processing_time,
                "morning_briefing": morning_briefing,
                "agent_performance": self._calculate_agent_performance(agent_results)
            }
            
        except Exception as e:
            self.logger.error(f"NightShift analysis failed: {e}")
            return {
                "status": "ERROR",
                "error_message": str(e),
                "processing_time_seconds": (datetime.now() - start_time).total_seconds()
            }
    
    async def _synthesize_briefing(self, agent_results: Dict[str, Any], 
                                 routes: List[str]) -> Dict[str, Any]:
        """Synthesize agent outputs into comprehensive morning briefing"""
        
        all_alerts = []
        
        # Collect all alerts from agents
        for agent_name, results in agent_results.items():
            if isinstance(results, list):
                all_alerts.extend(results)
            else:
                self.logger.warning(f"Agent {agent_name} returned error: {results}")
        
        # Sort alerts by priority and confidence
        all_alerts.sort(key=lambda x: (x.priority.value, -x.confidence_score))
        
        # Generate executive summary
        executive_summary = self._generate_executive_summary(all_alerts)
        
        # Create priority action items
        priority_actions = [alert for alert in all_alerts if alert.priority in [AlertPriority.CRITICAL, AlertPriority.HIGH]][:10]
        
        # Generate insights by category
        insights_by_category = {
            "external_events": [a for a in all_alerts if a.agent_name == "SurgeEventDetector"],
            "booking_patterns": [a for a in all_alerts if a.agent_name == "AdvanceBookingCurveAlerting"],
            "market_dynamics": [a for a in all_alerts if a.agent_name == "ElasticityChangeAlert"]
        }
        
        return {
            "generated_at": datetime.now().isoformat(),
            "executive_summary": executive_summary,
            "priority_actions": [asdict(action) for action in priority_actions],
            "insights_by_category": {k: [asdict(alert) for alert in v] for k, v in insights_by_category.items()},
            "total_alerts": len(all_alerts),
            "routes_analyzed": routes,
            "agent_performance": {name: len([a for a in all_alerts if a.agent_name == name.replace('_', '').title() + 'Agent']) 
                                for name in ['surge_events', 'booking_curves', 'elasticity_changes']}
        }
    
    def _generate_executive_summary(self, alerts: List[IntelligenceAlert]) -> Dict[str, Any]:
        """Generate high-level executive summary"""
        
        if not alerts:
            return {
                "status": "NORMAL",
                "key_message": "No significant market anomalies detected. Standard monitoring in place.",
                "total_revenue_impact": 0,
                "actions_required": 0
            }
        
        # Calculate aggregate metrics
        total_revenue_impact = sum(alert.revenue_impact_eur for alert in alerts)
        critical_alerts = len([a for a in alerts if a.priority == AlertPriority.CRITICAL])
        high_alerts = len([a for a in alerts if a.priority == AlertPriority.HIGH])
        
        # Determine overall status
        if critical_alerts > 0:
            status = "ATTENTION_REQUIRED"
        elif high_alerts > 2:
            status = "MONITOR_CLOSELY"
        else:
            status = "NORMAL_OPERATIONS"
        
        # Generate key message
        key_insights = []
        if critical_alerts > 0:
            key_insights.append(f"{critical_alerts} critical situation{'s' if critical_alerts > 1 else ''} requiring immediate action")
        if total_revenue_impact > 100000:
            key_insights.append(f"€{total_revenue_impact:,} total revenue opportunity identified")
        
        key_message = ". ".join(key_insights) if key_insights else "Market conditions stable across monitored routes."
        
        return {
            "status": status,
            "key_message": key_message,
            "total_revenue_impact": total_revenue_impact,
            "critical_alerts": critical_alerts,
            "high_priority_alerts": high_alerts,
            "actions_required": critical_alerts + high_alerts,
            "confidence_score": np.mean([alert.confidence_score for alert in alerts]) if alerts else 0.8
        }
    
    def _calculate_agent_performance(self, agent_results: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate performance metrics for each agent"""
        
        performance = {}
        
        for agent_name, results in agent_results.items():
            if isinstance(results, list):
                alerts = results
                performance[agent_name] = {
                    "alerts_generated": len(alerts),
                    "avg_confidence": np.mean([a.confidence_score for a in alerts]) if alerts else 0.0,
                    "revenue_impact": sum(a.revenue_impact_eur for a in alerts),
                    "status": "ACTIVE"
                }
            else:
                performance[agent_name] = {
                    "alerts_generated": 0,
                    "avg_confidence": 0.0,
                    "revenue_impact": 0,
                    "status": "ERROR",
                    "error": str(results)
                }
        
        return performance

# ============================================================================
# USAGE EXAMPLE AND TESTING
# ============================================================================

async def main():
    """Example usage of the Velociti Intelligence Agents"""
    
    # Initialize context
    context = AgentContext(
        airline_code="EZY",
        base_currency="EUR",
        operating_region="Europe"
    )
    
    # Initialize orchestrator
    orchestrator = VelocitiAgentOrchestrator(context)
    
    # Define routes to analyze
    test_routes = ["LGW-BCN", "LGW-MAD", "LGW-CDG", "LGW-PMI", "LGW-AGP"]
    
    print("🚀 Starting Velociti NightShift Intelligence Analysis...")
    print(f"📊 Analyzing {len(test_routes)} routes: {', '.join(test_routes)}")
    print("=" * 80)
    
    # Run the analysis
    results = await orchestrator.run_night_shift_analysis(test_routes)
    
    if results["status"] == "SUCCESS":
        briefing = results["morning_briefing"]
        
        print("📋 EXECUTIVE SUMMARY")
        print("-" * 40)
        summary = briefing["executive_summary"]
        print(f"Status: {summary['status']}")
        print(f"Key Message: {summary['key_message']}")
        print(f"Total Revenue Impact: €{summary['total_revenue_impact']:,}")
        print(f"Actions Required: {summary['actions_required']}")
        print()
        
        print("🚨 PRIORITY ACTIONS")
        print("-" * 40)
        for i, action in enumerate(briefing["priority_actions"][:3], 1):
            print(f"{i}. [{action['priority']}] {action['title']}")
            print(f"   Route: {', '.join(action['affected_routes'])}")
            print(f"   Impact: €{action['revenue_impact_eur']:,}")
            print(f"   Confidence: {action['confidence_score']:.0%}")
            print(f"   Action Required: {action['time_to_act']}")
            print()
        
        print("🎯 AGENT PERFORMANCE")
        print("-" * 40)
        for agent, perf in results["agent_performance"].items():
            print(f"{agent.replace('_', ' ').title()}: {perf['alerts_generated']} alerts, €{perf['revenue_impact']:,} impact")
        
        print(f"\n⏱️  Processing completed in {results['processing_time_seconds']:.2f} seconds")
        
    else:
        print(f"❌ Analysis failed: {results['error_message']}")

if __name__ == "__main__":
    # Set up logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Run the example
    asyncio.run(main())